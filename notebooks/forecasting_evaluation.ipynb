{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Forecasting Model Evaluation\n",
                "\n",
                "Comprehensive evaluation of LSTM, GRU, and Transformer forecasting models.\n",
                "\n",
                "Aligned with **docs/metrics_and_evaluation.md** specifications:\n",
                "- MAE reduction ≥15% vs ARIMA baseline\n",
                "- Directional Accuracy ≥55% (1-day) / ≥53% (3-day) / ≥52% (7-day)\n",
                "- RMSE reduction ≥12%\n",
                "- Sharpe ratio ≥1.0\n",
                "- Inference latency ≤300ms p95"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import torch\n",
                "from scipy import stats\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "from backend.utils.config import config\n",
                "from backend.utils.dataset import ForecastingDataset\n",
                "from backend.utils.preprocessing import load_scaler_metadata\n",
                "from backend.utils.model_utils import load_latest_checkpoint, count_model_parameters, measure_inference_latency, get_device\n",
                "from backend.utils.baselines import ARIMABaseline, MovingAverageBaseline, ExponentialSmoothingBaseline, evaluate_baseline\n",
                "from backend.utils.metrics import compute_all_metrics, mean_absolute_error, root_mean_squared_error, directional_accuracy\n",
                "from backend.utils.trading_sim import TradingSimulator, backtest_model\n",
                "from models import LSTMForecaster, GRUForecaster, TransformerForecaster\n",
                "\n",
                "# Set plot style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set paths and parameters\n",
                "TICKER = 'RELIANCE.NS'  # Change as needed\n",
                "DATA_DIR = project_root / 'data' / 'processed' / TICKER\n",
                "CHECKPOINT_DIR = project_root / 'models' / 'checkpoints'\n",
                "\n",
                "# Find latest processed data\n",
                "subdirs = [d for d in DATA_DIR.iterdir() if d.is_dir()]\n",
                "subdirs.sort(key=lambda d: d.stat().st_mtime, reverse=True)\n",
                "DATA_PATH = subdirs[0]\n",
                "\n",
                "print(f\"Ticker: {TICKER}\")\n",
                "print(f\"Data path: {DATA_PATH}\")\n",
                "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
                "\n",
                "# Device\n",
                "device = get_device()\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test dataset\n",
                "test_dataset = ForecastingDataset(DATA_PATH, split='test')\n",
                "\n",
                "# Load metadata\n",
                "import json\n",
                "with open(DATA_PATH / 'metadata.json', 'r') as f:\n",
                "    metadata = json.load(f)\n",
                "\n",
                "print(f\"Test samples: {len(test_dataset)}\")\n",
                "print(f\"Input features: {len(metadata['feature_names'])}\")\n",
                "print(f\"Date range: {metadata.get('split_dates', {})}\")\n",
                "print(f\"\\nFeature names: {metadata['feature_names'][:5]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load models\n",
                "input_dim = len(metadata['feature_names'])\n",
                "models_dict = {}\n",
                "\n",
                "# LSTM\n",
                "try:\n",
                "    lstm_path, lstm_meta = load_latest_checkpoint(CHECKPOINT_DIR, 'lstm', TICKER)\n",
                "    lstm_model = LSTMForecaster(input_dim=input_dim)\n",
                "    lstm_model.load_checkpoint(lstm_path)\n",
                "    lstm_model.to(device)\n",
                "    lstm_model.eval()\n",
                "    models_dict['LSTM'] = lstm_model\n",
                "    print(f\"Loaded LSTM: {count_model_parameters(lstm_model):,} parameters\")\n",
                "except FileNotFoundError:\n",
                "    print(\"LSTM checkpoint not found\")\n",
                "\n",
                "# GRU\n",
                "try:\n",
                "    gru_path, gru_meta = load_latest_checkpoint(CHECKPOINT_DIR, 'gru', TICKER)\n",
                "    gru_model = GRUForecaster(input_dim=input_dim)\n",
                "    gru_model.load_checkpoint(gru_path)\n",
                "    gru_model.to(device)\n",
                "    gru_model.eval()\n",
                "    models_dict['GRU'] = gru_model\n",
                "    print(f\"Loaded GRU: {count_model_parameters(gru_model):,} parameters\")\n",
                "except FileNotFoundError:\n",
                "    print(\"GRU checkpoint not found\")\n",
                "\n",
                "# Transformer\n",
                "try:\n",
                "    transformer_path, transformer_meta = load_latest_checkpoint(CHECKPOINT_DIR, 'transformer', TICKER)\n",
                "    transformer_model = TransformerForecaster(input_dim=input_dim)\n",
                "    transformer_model.load_checkpoint(transformer_path)\n",
                "    transformer_model.to(device)\n",
                "    transformer_model.eval()\n",
                "    models_dict['Transformer'] = transformer_model\n",
                "    print(f\"Loaded Transformer: {count_model_parameters(transformer_model):,} parameters\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Transformer checkpoint not found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Baseline Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load train data for baseline fitting\n",
                "train_targets = np.load(DATA_PATH / 'train_targets.npy')\n",
                "test_targets = np.load(DATA_PATH / 'test_targets.npy')\n",
                "\n",
                "baselines_dict = {}\n",
                "\n",
                "# ARIMA\n",
                "try:\n",
                "    arima = ARIMABaseline()\n",
                "    results = evaluate_baseline(arima, train_targets, test_targets, horizon=1)\n",
                "    baselines_dict['ARIMA'] = {'model': arima, 'metrics': results}\n",
                "    print(f\"ARIMA - MAE: {results['mae']:.4f}, RMSE: {results['rmse']:.4f}\")\n",
                "except Exception as e:\n",
                "    print(f\"ARIMA failed: {e}\")\n",
                "\n",
                "# Moving Average\n",
                "ma = MovingAverageBaseline(window=20)\n",
                "results = evaluate_baseline(ma, train_targets, test_targets, horizon=1)\n",
                "baselines_dict['Moving Average'] = {'model': ma, 'metrics': results}\n",
                "print(f\"Moving Average - MAE: {results['mae']:.4f}, RMSE: {results['rmse']:.4f}\")\n",
                "\n",
                "# Exponential Smoothing\n",
                "try:\n",
                "    es = ExponentialSmoothingBaseline()\n",
                "    results = evaluate_baseline(es, train_targets, test_targets, horizon=1)\n",
                "    baselines_dict['Exp Smoothing'] = {'model': es, 'metrics': results}\n",
                "    print(f\"Exp Smoothing - MAE: {results['mae']:.4f}, RMSE: {results['rmse']:.4f}\")\n",
                "except Exception as e:\n",
                "    print(f\"Exponential Smoothing failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Generate Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate predictions for all models\n",
                "predictions_dict = {}\n",
                "\n",
                "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
                "\n",
                "for model_name, model in models_dict.items():\n",
                "    all_preds = []\n",
                "    all_targets = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for features, targets in test_loader:\n",
                "            features = features.to(device)\n",
                "            preds = model(features)\n",
                "            all_preds.append(preds.cpu().numpy())\n",
                "            all_targets.append(targets.cpu().numpy())\n",
                "    \n",
                "    preds = np.concatenate(all_preds)\n",
                "    targets = np.concatenate(all_targets)\n",
                "    \n",
                "    # Take first forecast step if multi-step\n",
                "    if preds.ndim > 1 and preds.shape[1] > 1:\n",
                "        preds = preds[:, 0]\n",
                "    if targets.ndim > 1:\n",
                "        targets = targets[:, 0]\n",
                "    \n",
                "    predictions_dict[model_name] = {'predictions': preds, 'actuals': targets}\n",
                "    print(f\"{model_name}: {len(preds)} predictions generated\")\n",
                "\n",
                "# Create results DataFrame\n",
                "results_df = pd.DataFrame({\n",
                "    'actual': predictions_dict[list(models_dict.keys())[0]]['actuals']\n",
                "})\n",
                "\n",
                "for model_name, data in predictions_dict.items():\n",
                "    results_df[f'{model_name.lower()}_pred'] = data['predictions']\n",
                "\n",
                "# Add baseline predictions\n",
                "for baseline_name, baseline_data in baselines_dict.items():\n",
                "    baseline_preds = []\n",
                "    for i in range(len(test_targets)):\n",
                "        pred = baseline_data['model'].predict(1)\n",
                "        baseline_preds.append(pred[0] if isinstance(pred, np.ndarray) else pred)\n",
                "    results_df[f'{baseline_name.lower().replace(\" \", \"_\")}_pred'] = baseline_preds[:len(results_df)]\n",
                "\n",
                "print(\"\\nResults DataFrame:\")\n",
                "print(results_df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Metrics Computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute metrics for all models\n",
                "metrics_results = {}\n",
                "\n",
                "for model_name, data in predictions_dict.items():\n",
                "    metrics = compute_all_metrics(data['actuals'], data['predictions'])\n",
                "    metrics_results[model_name] = metrics\n",
                "\n",
                "# Add baseline metrics\n",
                "for baseline_name, baseline_data in baselines_dict.items():\n",
                "    metrics_results[baseline_name] = baseline_data['metrics']\n",
                "\n",
                "# Create comparison table\n",
                "metrics_df = pd.DataFrame(metrics_results).T\n",
                "metrics_df = metrics_df.round(4)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"METRICS COMPARISON\")\n",
                "print(\"=\"*80)\n",
                "print(metrics_df)\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Highlight best performers\n",
                "print(\"\\nBest Performers:\")\n",
                "for col in metrics_df.columns:\n",
                "    if col.startswith('da_'):\n",
                "        best = metrics_df[col].idxmax()\n",
                "        print(f\"{col}: {best} ({metrics_df.loc[best, col]:.4f})\")\n",
                "    else:\n",
                "        best = metrics_df[col].idxmin()\n",
                "        print(f\"{col}: {best} ({metrics_df.loc[best, col]:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Statistical Significance Tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paired t-test for MAE against ARIMA baseline\n",
                "if 'ARIMA' in baselines_dict:\n",
                "    arima_preds = baselines_dict['ARIMA']['model'].predict(len(test_targets))\n",
                "    arima_errors = np.abs(test_targets[:len(arima_preds)] - arima_preds)\n",
                "    \n",
                "    print(\"\\nStatistical Significance (Paired t-test vs ARIMA):\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    for model_name, data in predictions_dict.items():\n",
                "        model_errors = np.abs(data['actuals'] - data['predictions'])\n",
                "        min_len = min(len(arima_errors), len(model_errors))\n",
                "        \n",
                "        t_stat, p_value = stats.ttest_rel(arima_errors[:min_len], model_errors[:min_len])\n",
                "        \n",
                "        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
                "        improvement = ((arima_errors[:min_len].mean() - model_errors[:min_len].mean()) / arima_errors[:min_len].mean()) * 100\n",
                "        \n",
                "        print(f\"{model_name:15s}: t={t_stat:7.3f}, p={p_value:.4f} {significance:3s} | Improvement: {improvement:+6.2f}%\")\n",
                "    \n",
                "    print(\"-\" * 60)\n",
                "    print(\"Significance: *** p<0.001, ** p<0.01, * p<0.05, ns=not significant\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot actual vs predicted\n",
                "fig, axes = plt.subplots(len(models_dict), 1, figsize=(14, 4*len(models_dict)))\n",
                "if len(models_dict) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "for idx, (model_name, data) in enumerate(predictions_dict.items()):\n",
                "    ax = axes[idx]\n",
                "    ax.plot(data['actuals'][:100], label='Actual', linewidth=2)\n",
                "    ax.plot(data['predictions'][:100], label='Predicted', linewidth=2, alpha=0.7)\n",
                "    ax.set_title(f'{model_name} Predictions vs Actual (first 100 samples)')\n",
                "    ax.set_xlabel('Sample')\n",
                "    ax.set_ylabel('Value')\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Error distributions\n",
                "fig, axes = plt.subplots(1, len(models_dict), figsize=(6*len(models_dict), 4))\n",
                "if len(models_dict) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "for idx, (model_name, data) in enumerate(predictions_dict.items()):\n",
                "    errors = data['actuals'] - data['predictions']\n",
                "    axes[idx].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
                "    axes[idx].set_title(f'{model_name} Error Distribution')\n",
                "    axes[idx].set_xlabel('Prediction Error')\n",
                "    axes[idx].set_ylabel('Frequency')\n",
                "    axes[idx].axvline(0, color='red', linestyle='--', linewidth=2)\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Latency Measurement"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Measure inference latency\n",
                "sample_input = torch.randn(1, 60, input_dim).to(device)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"INFERENCE LATENCY (Target: ≤300ms p95)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "latency_results = {}\n",
                "\n",
                "for model_name, model in models_dict.items():\n",
                "    mean_ms, p95_ms, p99_ms = measure_inference_latency(model, sample_input, num_runs=100)\n",
                "    latency_results[model_name] = {'mean': mean_ms, 'p95': p95_ms, 'p99': p99_ms}\n",
                "    \n",
                "    status = \"✓ PASS\" if p95_ms <= 300 else \"✗ FAIL\"\n",
                "    print(f\"{model_name:15s}: Mean={mean_ms:6.2f}ms, P95={p95_ms:6.2f}ms, P99={p99_ms:6.2f}ms {status}\")\n",
                "\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Final Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create evaluation summary\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"PHASE 3 EVALUATION SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Check targets from docs/metrics_and_evaluation.md\n",
                "print(\"\\nTarget Achievements:\")\n",
                "print(\"-\" * 80)\n",
                "\n",
                "for model_name in models_dict.keys():\n",
                "    print(f\"\\n{model_name}:\")\n",
                "    \n",
                "    if model_name in metrics_results:\n",
                "        metrics = metrics_results[model_name]\n",
                "        \n",
                "        # MAE reduction vs ARIMA\n",
                "        if 'ARIMA' in baselines_dict:\n",
                "            arima_mae = baselines_dict['ARIMA']['metrics']['mae']\n",
                "            reduction = ((arima_mae - metrics['mae']) / arima_mae) * 100\n",
                "            status = \"✓\" if reduction >= 15 else \"✗\"\n",
                "            print(f\"  MAE reduction vs ARIMA:  {reduction:6.2f}% {status} (target: ≥15%)\")\n",
                "        \n",
                "        # Directional Accuracy\n",
                "        da_1 = metrics.get('da_1day', 0)\n",
                "        status_1 = \"✓\" if da_1 >= 0.55 else \"✗\"\n",
                "        print(f\"  DA (1-day):              {da_1:6.2%} {status_1} (target: ≥55%)\")\n",
                "        \n",
                "        da_3 = metrics.get('da_3day', 0)\n",
                "        status_3 = \"✓\" if da_3 >= 0.53 else \"✗\"\n",
                "        print(f\"  DA (3-day):              {da_3:6.2%} {status_3} (target: ≥53%)\")\n",
                "        \n",
                "        da_7 = metrics.get('da_7day', 0)\n",
                "        status_7 = \"✓\" if da_7 >= 0.52 else \"✗\"\n",
                "        print(f\"  DA (7-day):              {da_7:6.2%} {status_7} (target: ≥52%)\")\n",
                "    \n",
                "    # Latency\n",
                "    if model_name in latency_results:\n",
                "        p95 = latency_results[model_name]['p95']\n",
                "        status = \"✓\" if p95 <= 300 else \"✗\"\n",
                "        print(f\"  Inference latency (P95): {p95:6.2f}ms {status} (target: ≤300ms)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"Evaluation complete! Review metrics above.\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Export Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export results to CSV\n",
                "output_dir = project_root / 'results'\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
                "\n",
                "# Save metrics\n",
                "metrics_df.to_csv(output_dir / f'metrics_{TICKER}_{timestamp}.csv')\n",
                "print(f\"Metrics saved to: {output_dir / f'metrics_{TICKER}_{timestamp}.csv'}\")\n",
                "\n",
                "# Save predictions\n",
                "results_df.to_csv(output_dir / f'predictions_{TICKER}_{timestamp}.csv', index=False)\n",
                "print(f\"Predictions saved to: {output_dir / f'predictions_{TICKER}_{timestamp}.csv'}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}