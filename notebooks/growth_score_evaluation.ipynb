{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Growth Scorer Evaluation\n",
                "\n",
                "This notebook evaluates the Growth Scorer ensemble against the targets defined in `docs/metrics_and_evaluation.md`.\n",
                "\n",
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from models.growth_scorer import GrowthScorer\n",
                "from backend.utils.growth_data import (\n",
                "    load_fundamentals,\n",
                "    load_technical_indicators,\n",
                "    load_prices,\n",
                "    merge_growth_features,\n",
                "    compute_forward_returns,\n",
                "    engineer_growth_features,\n",
                "    split_growth_data,\n",
                "    validate_growth_features\n",
                ")\n",
                "from backend.utils.growth_metrics import (\n",
                "    compute_all_growth_metrics,\n",
                "    spearman_correlation,\n",
                "    top_k_precision,\n",
                "    compute_excess_return,\n",
                "    information_ratio,\n",
                "    decile_analysis,\n",
                "    hit_rate\n",
                ")\n",
                "from backend.utils.config import config\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set_style('whitegrid')\n",
                "print(\"Imports complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model and Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load checkpoint - update path to your trained model\n",
                "checkpoint_path = os.environ.get('GROWTH_CHECKPOINT', 'models/checkpoints/growth_scorer_latest.pkl')\n",
                "\n",
                "if not os.path.exists(checkpoint_path):\n",
                "    # Find the most recent checkpoint\n",
                "    ckpt_dir = 'models/checkpoints'\n",
                "    if os.path.exists(ckpt_dir):\n",
                "        files = [f for f in os.listdir(ckpt_dir) if f.startswith('growth_scorer_') and f.endswith('.pkl')]\n",
                "        if files:\n",
                "            checkpoint_path = os.path.join(ckpt_dir, sorted(files)[-1])\n",
                "            print(f\"Using most recent checkpoint: {checkpoint_path}\")\n",
                "        else:\n",
                "            print(\"No growth scorer checkpoints found. Please train a model first using scripts/train_growth_scorer.py\")\n",
                "            checkpoint_path = None\n",
                "    else:\n",
                "        checkpoint_path = None\n",
                "\n",
                "if checkpoint_path and os.path.exists(checkpoint_path):\n",
                "    model, metadata = GrowthScorer.load_checkpoint(checkpoint_path)\n",
                "    print(f\"Loaded model from: {checkpoint_path}\")\n",
                "    print(f\"Model type: {metadata.get('args', {}).get('model_type', 'unknown')}\")\n",
                "    print(f\"Number of features: {metadata.get('num_features', len(model.feature_names))}\")\n",
                "    print(f\"Split type: {metadata.get('split_type', 'unknown')}\")\n",
                "else:\n",
                "    print(\"ERROR: No checkpoint found. Please train a model first.\")\n",
                "    model = None\n",
                "    metadata = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load data\n",
                "fundamentals_dir = metadata.get('args', {}).get('fundamentals_dir', 'data/raw/fundamentals')\n",
                "technical_dir = metadata.get('args', {}).get('technical_dir', 'data/processed')\n",
                "price_dir = metadata.get('args', {}).get('price_dir', 'data/raw/prices')\n",
                "horizon_days = metadata.get('args', {}).get('horizon_days', 60)\n",
                "\n",
                "print(f\"Loading data from:\")\n",
                "print(f\"  Fundamentals: {fundamentals_dir}\")\n",
                "print(f\"  Technicals: {technical_dir}\")\n",
                "print(f\"  Prices: {price_dir}\")\n",
                "print(f\"  Horizon: {horizon_days} days\")\n",
                "\n",
                "# Load price data\n",
                "price_df = load_prices(price_dir)\n",
                "print(f\"Loaded {len(price_df)} price records\")\n",
                "\n",
                "# Load fundamentals\n",
                "fund_df = load_fundamentals(fundamentals_dir)\n",
                "print(f\"Loaded {len(fund_df)} fundamental records\")\n",
                "\n",
                "# Load technical indicators\n",
                "tech_df = load_technical_indicators(technical_dir, price_df=price_df)\n",
                "print(f\"Loaded {len(tech_df)} technical indicator records\")\n",
                "\n",
                "# Compute forward returns\n",
                "price_df['fwd_return'] = compute_forward_returns(price_df, horizon_days)\n",
                "\n",
                "# Merge features\n",
                "if not fund_df.empty:\n",
                "    merged_df = merge_growth_features(fund_df, tech_df, price_df)\n",
                "else:\n",
                "    merged_df = tech_df.merge(\n",
                "        price_df[['ticker', 'date', 'fwd_return']], \n",
                "        on=['ticker', 'date'], \n",
                "        how='inner'\n",
                "    )\n",
                "\n",
                "merged_df = merged_df.dropna(subset=['fwd_return'])\n",
                "print(f\"Merged dataset size: {len(merged_df)} records\")\n",
                "\n",
                "# Split data (use same method as training)\n",
                "train_df, val_df, test_df = split_growth_data(merged_df)\n",
                "print(f\"Split sizes: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Engineer features\n",
                "print(\"Engineering features...\")\n",
                "train_engineered, feature_names, scalers = engineer_growth_features(train_df, is_train=True)\n",
                "val_engineered, _, _ = engineer_growth_features(val_df, is_train=False, scalers=scalers)\n",
                "test_engineered, _, _ = engineer_growth_features(test_df, is_train=False, scalers=scalers)\n",
                "\n",
                "# Extract feature matrices and targets\n",
                "X_val = val_engineered[feature_names].values\n",
                "y_val = val_df['fwd_return'].values\n",
                "\n",
                "X_test = test_engineered[feature_names].values\n",
                "y_test = test_df['fwd_return'].values\n",
                "\n",
                "print(f\"Validation feature shape: {X_val.shape}\")\n",
                "print(f\"Test feature shape: {X_test.shape}\")\n",
                "\n",
                "# Generate predictions\n",
                "if model:\n",
                "    val_scores = model.predict(X_val)\n",
                "    test_scores = model.predict(X_test)\n",
                "    print(f\"Predictions generated\")\n",
                "    print(f\"Val scores range: [{val_scores.min():.4f}, {val_scores.max():.4f}]\")\n",
                "    print(f\"Test scores range: [{test_scores.min():.4f}, {test_scores.max():.4f}]\")\n",
                "else:\n",
                "    print(\"ERROR: No model loaded\")\n",
                "    val_scores = None\n",
                "    test_scores = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Validation Metrics\n",
                "\n",
                "Target: Spearman >= 0.30, Top-10 >= 70%, Excess Return >= 3%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80124161",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute validation metrics\n",
                "if val_scores is not None:\n",
                "    val_metrics = compute_all_growth_metrics(val_scores, y_val, benchmark_return=0.0)\n",
                "    \n",
                "    print(\"VALIDATION METRICS\")\n",
                "    print(\"=\" * 60)\n",
                "    print(f\"Spearman Correlation: {val_metrics['correlation']:.4f} (p={val_metrics.get('spearman_p_value', 1.0):.4f})\")\n",
                "    print(f\"  Target: >= 0.30\")\n",
                "    print(f\"  Status: {'PASS ✓' if val_metrics['correlation'] >= 0.30 else 'FAIL ✗'}\")\n",
                "    print()\n",
                "    \n",
                "    if 'top_k_precision' in val_metrics:\n",
                "        print(\"Top-K Precision:\")\n",
                "        for k, prec in sorted(val_metrics['top_k_precision'].items()):\n",
                "            status = 'PASS ✓' if (k == 10 and prec >= 0.70) else ''\n",
                "            print(f\"  Top-{k}: {prec:.4f} {status}\")\n",
                "        print(f\"  Target (Top-10): >= 0.70\")\n",
                "    print()\n",
                "    \n",
                "    print(f\"Excess Return: {val_metrics['excess_return']:.4f} ({val_metrics['excess_return']*100:.2f}%)\")\n",
                "    print(f\"  p-value: {val_metrics.get('excess_p_value', 1.0):.4f}\")\n",
                "    print(f\"  Target: >= 0.03 (3%)\")\n",
                "    print(f\"  Status: {'PASS ✓' if val_metrics['excess_return'] >= 0.03 else 'FAIL ✗'}\")\n",
                "    print()\n",
                "    \n",
                "    if 'decile_spread' in val_metrics:\n",
                "        print(f\"Decile Spread (Top - Bottom): {val_metrics['decile_spread']:.4f} ({val_metrics['decile_spread']*100:.2f}%)\")\n",
                "    print(\"=\" * 60)\n",
                "else:\n",
                "    print(\"No validation scores available\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d980c3ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "## 4. Test Set Metrics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "18850291",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute test metrics\n",
                "if test_scores is not None:\n",
                "    test_metrics = compute_all_growth_metrics(test_scores, y_test, benchmark_return=0.0)\n",
                "    \n",
                "    print(\"TEST METRICS\")\n",
                "    print(\"=\" * 60)\n",
                "    print(f\"Spearman Correlation: {test_metrics['correlation']:.4f} (p={test_metrics.get('spearman_p_value', 1.0):.4f})\")\n",
                "    print(f\"  Target: >= 0.30\")\n",
                "    print(f\"  Status: {'PASS ✓' if test_metrics['correlation'] >= 0.30 else 'FAIL ✗'}\")\n",
                "    print()\n",
                "    \n",
                "    if 'top_k_precision' in test_metrics:\n",
                "        print(\"Top-K Precision:\")\n",
                "        for k, prec in sorted(test_metrics['top_k_precision'].items()):\n",
                "            status = 'PASS ✓' if (k == 10 and prec >= 0.70) else ''\n",
                "            print(f\"  Top-{k}: {prec:.4f} {status}\")\n",
                "        print(f\"  Target (Top-10): >= 0.70\")\n",
                "    print()\n",
                "    \n",
                "    print(f\"Excess Return: {test_metrics['excess_return']:.4f} ({test_metrics['excess_return']*100:.2f}%)\")\n",
                "    print(f\"  p-value: {test_metrics.get('excess_p_value', 1.0):.4f}\")\n",
                "    print(f\"  Target: >= 0.03 (3%)\")\n",
                "    print(f\"  Status: {'PASS ✓' if test_metrics['excess_return'] >= 0.03 else 'FAIL ✗'}\")\n",
                "    print()\n",
                "    \n",
                "    if 'decile_spread' in test_metrics:\n",
                "        print(f\"Decile Spread (Top - Bottom): {test_metrics['decile_spread']:.4f} ({test_metrics['decile_spread']*100:.2f}%)\")\n",
                "    print(\"=\" * 60)\n",
                "else:\n",
                "    print(\"No test scores available\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "88ff3db7",
            "metadata": {},
            "outputs": [],
            "source": [
                "## 5. Visualizations\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "950163d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation scatter plot\n",
                "if test_scores is not None:\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Validation\n",
                "    ax1.scatter(val_scores, y_val, alpha=0.5, s=20)\n",
                "    ax1.set_xlabel('Predicted Growth Score')\n",
                "    ax1.set_ylabel('Realized Return')\n",
                "    ax1.set_title(f'Validation: Score vs Return (ρ={val_metrics[\"correlation\"]:.3f})')\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Add trend line\n",
                "    z = np.polyfit(val_scores, y_val, 1)\n",
                "    p = np.poly1d(z)\n",
                "    x_line = np.linspace(val_scores.min(), val_scores.max(), 100)\n",
                "    ax1.plot(x_line, p(x_line), 'r--', alpha=0.8, linewidth=2)\n",
                "    \n",
                "    # Test\n",
                "    ax2.scatter(test_scores, y_test, alpha=0.5, s=20, color='orange')\n",
                "    ax2.set_xlabel('Predicted Growth Score')\n",
                "    ax2.set_ylabel('Realized Return')\n",
                "    ax2.set_title(f'Test: Score vs Return (ρ={test_metrics[\"correlation\"]:.3f})')\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Add trend line\n",
                "    z = np.polyfit(test_scores, y_test, 1)\n",
                "    p = np.poly1d(z)\n",
                "    x_line = np.linspace(test_scores.min(), test_scores.max(), 100)\n",
                "    ax2.plot(x_line, p(x_line), 'r--', alpha=0.8, linewidth=2)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('growth_scorer_correlation.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No scores available for plotting\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3a4f34a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decile analysis\n",
                "if test_scores is not None:\n",
                "    deciles = decile_analysis(test_scores, y_test, num_deciles=10)\n",
                "    \n",
                "    if not deciles.empty:\n",
                "        fig, ax = plt.subplots(figsize=(10, 6))\n",
                "        \n",
                "        x_pos = np.arange(len(deciles))\n",
                "        colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(deciles)))\n",
                "        \n",
                "        bars = ax.bar(x_pos, deciles['mean'], yerr=deciles['std'], \n",
                "                      capsize=5, color=colors, alpha=0.8, edgecolor='black')\n",
                "        \n",
                "        ax.set_xlabel('Decile (1=Highest Score, 10=Lowest Score)', fontsize=12)\n",
                "        ax.set_ylabel('Mean Realized Return', fontsize=12)\n",
                "        ax.set_title('Decile Analysis: Mean Return by Predicted Score Decile', fontsize=14, fontweight='bold')\n",
                "        ax.set_xticks(x_pos)\n",
                "        ax.set_xticklabels([f'D{i}' for i in deciles.index])\n",
                "        ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
                "        ax.grid(axis='y', alpha=0.3)\n",
                "        \n",
                "        # Add count labels\n",
                "        for i, (idx, row) in enumerate(deciles.iterrows()):\n",
                "            ax.text(i, row['mean'] + row['std'] + 0.005, f\"n={int(row['count'])}\", \n",
                "                   ha='center', va='bottom', fontsize=8)\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        plt.savefig('growth_scorer_deciles.png', dpi=300, bbox_inches='tight')\n",
                "        plt.show()\n",
                "        \n",
                "        print(\"\\nDecile Statistics:\")\n",
                "        print(deciles.round(4))\n",
                "    else:\n",
                "        print(\"Insufficient data for decile analysis\")\n",
                "else:\n",
                "    print(\"No scores available for decile analysis\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a6fbe0a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "if model and hasattr(model, 'get_feature_importances'):\n",
                "    importance_df = model.get_feature_importances()\n",
                "    top_n = 20\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "    \n",
                "    top_features = importance_df.head(top_n)\n",
                "    y_pos = np.arange(len(top_features))\n",
                "    \n",
                "    ax.barh(y_pos, top_features['importance'], color='steelblue', alpha=0.8)\n",
                "    ax.set_yticks(y_pos)\n",
                "    ax.set_yticklabels(top_features['feature'])\n",
                "    ax.invert_yaxis()\n",
                "    ax.set_xlabel('Importance', fontsize=12)\n",
                "    ax.set_title(f'Top {top_n} Feature Importances', fontsize=14, fontweight='bold')\n",
                "    ax.grid(axis='x', alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('growth_scorer_feature_importance.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nTop {top_n} Features:\")\n",
                "    print(top_features.to_string(index=False))\n",
                "else:\n",
                "    print(\"Feature importances not available\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86ce66ec",
            "metadata": {},
            "outputs": [],
            "source": [
                "## 6. Summary and Conclusions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbf69811",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary table\n",
                "if test_scores is not None and val_scores is not None:\n",
                "    summary_data = {\n",
                "        'Metric': [\n",
                "            'Spearman Correlation',\n",
                "            'Top-10 Precision',\n",
                "            'Excess Return (%)',\n",
                "            'Decile Spread (%)'\n",
                "        ],\n",
                "        'Target': [\n",
                "            '≥ 0.30',\n",
                "            '≥ 0.70',\n",
                "            '≥ 3.0%',\n",
                "            'Positive'\n",
                "        ],\n",
                "        'Validation': [\n",
                "            f\"{val_metrics['correlation']:.4f}\",\n",
                "            f\"{val_metrics.get('top_k_precision', {}).get(10, 0):.4f}\",\n",
                "            f\"{val_metrics['excess_return']*100:.2f}%\",\n",
                "            f\"{val_metrics.get('decile_spread', 0)*100:.2f}%\"\n",
                "        ],\n",
                "        'Test': [\n",
                "            f\"{test_metrics['correlation']:.4f}\",\n",
                "            f\"{test_metrics.get('top_k_precision', {}).get(10, 0):.4f}\",\n",
                "            f\"{test_metrics['excess_return']*100:.2f}%\",\n",
                "            f\"{test_metrics.get('decile_spread', 0)*100:.2f}%\"\n",
                "        ],\n",
                "        'Val Status': [\n",
                "            '✓' if val_metrics['correlation'] >= 0.30 else '✗',\n",
                "            '✓' if val_metrics.get('top_k_precision', {}).get(10, 0) >= 0.70 else '✗',\n",
                "            '✓' if val_metrics['excess_return'] >= 0.03 else '✗',\n",
                "            '✓' if val_metrics.get('decile_spread', 0) > 0 else '✗'\n",
                "        ],\n",
                "        'Test Status': [\n",
                "            '✓' if test_metrics['correlation'] >= 0.30 else '✗',\n",
                "            '✓' if test_metrics.get('top_k_precision', {}).get(10, 0) >= 0.70 else '✗',\n",
                "            '✓' if test_metrics['excess_return'] >= 0.03 else '✗',\n",
                "            '✓' if test_metrics.get('decile_spread', 0) > 0 else '✗'\n",
                "        ]\n",
                "    }\n",
                "    \n",
                "    summary_df = pd.DataFrame(summary_data)\n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(\"GROWTH SCORER EVALUATION SUMMARY\")\n",
                "    print(\"=\" * 80)\n",
                "    print(summary_df.to_string(index=False))\n",
                "    print(\"=\" * 80)\n",
                "    \n",
                "    # Overall assessment\n",
                "    val_pass = all([\n",
                "        val_metrics['correlation'] >= 0.30,\n",
                "        val_metrics.get('top_k_precision', {}).get(10, 0) >= 0.70,\n",
                "        val_metrics['excess_return'] >= 0.03\n",
                "    ])\n",
                "    \n",
                "    test_pass = all([\n",
                "        test_metrics['correlation'] >= 0.30,\n",
                "        test_metrics.get('top_k_precision', {}).get(10, 0) >= 0.70,\n",
                "        test_metrics['excess_return'] >= 0.03\n",
                "    ])\n",
                "    \n",
                "    print(\"\\nOVERALL ASSESSMENT:\")\n",
                "    print(f\"Validation: {'PASS ✓' if val_pass else 'FAIL ✗'}\")\n",
                "    print(f\"Test:       {'PASS ✓' if test_pass else 'FAIL ✗'}\")\n",
                "    print(\"\\nThe model demonstrates {'strong' if test_pass else 'limited'} predictive power for growth scoring.\")\n",
                "else:\n",
                "    print(\"Insufficient data for summary\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
