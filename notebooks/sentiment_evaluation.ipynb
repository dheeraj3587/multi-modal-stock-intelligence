{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentiment Model Evaluation\n",
                "\n",
                "This notebook evaluates the FinBERT sentiment classifier against the targets defined in `docs/metrics_and_evaluation.md`.\n",
                "\n",
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import torch\n",
                "\n",
                "from transformers import AutoTokenizer\n",
                "from sklearn.metrics import classification_report as sk_classification_report, precision_recall_curve\n",
                "from scipy import stats\n",
                "\n",
                "from backend.utils.config import config\n",
                "from backend.utils.sentiment_data import (\n",
                "    load_news_articles,\n",
                "    load_prices_for_labeling,\n",
                "    create_labeled_dataset,\n",
                "    split_sentiment_data,\n",
                "    SentimentDataset,\n",
                ")\n",
                "from backend.utils.sentiment_metrics import (\n",
                "    compute_all_sentiment_metrics,\n",
                "    compute_confusion_matrix,\n",
                "    sentiment_price_correlation,\n",
                "    compare_to_baseline,\n",
                "    precision_score as sm_precision,\n",
                "    recall_score as sm_recall,\n",
                "    f1_score as sm_f1,\n",
                ")\n",
                "from models.sentiment_classifier import FinBERTSentimentClassifier\n",
                "\n",
                "%matplotlib inline\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model and Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load checkpoint\n",
                "checkpoint_path = os.environ.get('SENTIMENT_CHECKPOINT', 'models/checkpoints/finbert_sentiment_20241118_120000.pth')\n",
                "if not os.path.exists(checkpoint_path):\n",
                "    fallback = 'models/checkpoints/best_sentiment.pth'\n",
                "    print(f\"Checkpoint not found at {checkpoint_path}. Falling back to {fallback} (if exists).\")\n",
                "    checkpoint_path = fallback\n",
                "\n",
                "model, metadata = FinBERTSentimentClassifier.load_checkpoint(checkpoint_path, device=device)\n",
                "\n",
                "model_name = metadata.get('model_name', getattr(model, 'model_name', config.finbert_model_name))\n",
                "max_length = int(metadata.get('max_length', config.finbert_max_length))\n",
                "batch_size = int(metadata.get('batch_size', config.finbert_batch_size))\n",
                "labeling_strategy = metadata.get('labeling_strategy', 'price_change')\n",
                "test_start = metadata.get('test_start')\n",
                "test_end_date = metadata.get('test_end_date')\n",
                "\n",
                "# Tokenizer aligned with model\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# Log model details\n",
                "param_counts = model.count_parameters()\n",
                "trainable_params = param_counts.get('trainable', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
                "print(json.dumps({\n",
                "    \"checkpoint_path\": checkpoint_path,\n",
                "    \"model_name\": model_name,\n",
                "    \"max_length\": max_length,\n",
                "    \"batch_size\": batch_size,\n",
                "    \"trainable_params\": int(trainable_params),\n",
                "    \"metadata_keys\": list(metadata.keys())\n",
                "}, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d1e8298",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data (news and prices) and generate labels\n",
                "news_dir = str(config.get_raw_data_dir('news'))\n",
                "prices_dir = str(config.get_raw_data_dir('prices'))\n",
                "\n",
                "news_df = load_news_articles(news_dir, end_date=test_end_date)\n",
                "if news_df.empty:\n",
                "    print(\"Warning: No news loaded from\", news_dir)\n",
                "\n",
                "prices_df = load_prices_for_labeling(prices_dir)\n",
                "if prices_df.empty:\n",
                "    print(\"Warning: No prices loaded from\", prices_dir)\n",
                "\n",
                "labeled_df = create_labeled_dataset(news_df, price_df=prices_df, labeling_strategy=labeling_strategy)\n",
                "labeled_df['published_at'] = pd.to_datetime(labeled_df['published_at']).dt.normalize()\n",
                "\n",
                "# Preserve training-defined test window if available\n",
                "if test_start:\n",
                "    test_mask = labeled_df['published_at'] >= pd.to_datetime(test_start)\n",
                "    if test_end_date:\n",
                "        test_mask &= labeled_df['published_at'] <= pd.to_datetime(test_end_date)\n",
                "    test_df = labeled_df[test_mask].copy()\n",
                "else:\n",
                "    _, _, test_df = split_sentiment_data(labeled_df)\n",
                "\n",
                "# Extract inputs and targets\n",
                "test_texts = test_df['text'].tolist()\n",
                "test_labels = test_df['label'].astype(int).values\n",
                "\n",
                "print(f\"Test size: {len(test_labels)}\")\n",
                "print(\"Class distribution (%):\")\n",
                "print(pd.Series(test_labels).value_counts(normalize=True).sort_index().apply(lambda x: round(x * 100, 2)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b7c0b28a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inference: get predictions, confidence, and per-class probabilities\n",
                "import numpy as np\n",
                "\n",
                "def batched_predict_with_probs(model, tokenizer, texts, device, max_length=512, batch_size=32):\n",
                "    model.eval()\n",
                "    all_preds, all_confs, all_probs = [], [], []\n",
                "    for i in range(0, len(texts), batch_size):\n",
                "        batch_texts = texts[i:i+batch_size]\n",
                "        enc = tokenizer(\n",
                "            batch_texts,\n",
                "            padding=True,\n",
                "            truncation=True,\n",
                "            max_length=max_length,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        input_ids = enc['input_ids'].to(device)\n",
                "        attention_mask = enc['attention_mask'].to(device)\n",
                "        with torch.no_grad():\n",
                "            logits = model(input_ids, attention_mask)\n",
                "            probs = torch.softmax(logits, dim=1)\n",
                "            confs, preds = torch.max(probs, dim=1)\n",
                "        all_probs.append(probs.cpu().numpy())\n",
                "        all_confs.append(confs.cpu().numpy())\n",
                "        all_preds.append(preds.cpu().numpy())\n",
                "    return np.concatenate(all_preds), np.concatenate(all_confs), np.vstack(all_probs)\n",
                "\n",
                "y_pred, conf_max, probs_all = batched_predict_with_probs(\n",
                "    model, tokenizer, test_texts, device, max_length=max_length, batch_size=32\n",
                ")\n",
                "\n",
                "# Continuous sentiment score: Positive prob - Negative prob\n",
                "continuous_sentiment = probs_all[:, 0] - probs_all[:, 2] if probs_all.shape[1] >= 3 else conf_max\n",
                "\n",
                "pd.DataFrame({\n",
                "    'text': test_texts[:5],\n",
                "    'true': test_labels[:5],\n",
                "    'pred': y_pred[:5],\n",
                "    'conf': conf_max[:5]\n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Classification Metrics\n",
                "\n",
                "Target: Macro F1 >= 0.80"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75cb0a20",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification metrics and threshold checks\n",
                "prec = sm_precision(test_labels, y_pred)\n",
                "rec = sm_recall(test_labels, y_pred)\n",
                "f1d = sm_f1(test_labels, y_pred)\n",
                "\n",
                "print(\"Classification report (sklearn):\")\n",
                "print(sk_classification_report(test_labels, y_pred, target_names=['Positive','Neutral','Negative'], zero_division=0))\n",
                "\n",
                "print(\"Precision per-class:\", prec['per_class'], \"macro:\", round(prec['macro'], 4))\n",
                "print(\"Recall per-class:\", rec['per_class'], \"macro:\", round(rec['macro'], 4))\n",
                "print(\"F1 per-class:\", f1d['per_class'], \"macro:\", round(f1d['macro'], 4))\n",
                "\n",
                "pass_status = (\n",
                "    f1d['macro'] >= 0.80 and\n",
                "    all(np.array(prec['per_class']) >= 0.75) and\n",
                "    all(np.array(rec['per_class']) >= 0.75)\n",
                ")\n",
                "print(\"Pass thresholds (Section 2.1):\", pass_status)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Confusion Matrix\n",
                "\n",
                "Target: No off-diagonal cell > 15%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b635f89c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix heatmap\n",
                "cm_dict = compute_confusion_matrix(test_labels, y_pred)\n",
                "cm_norm = np.array(cm_dict['normalized_matrix'])\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(\n",
                "    cm_norm,\n",
                "    annot=True,\n",
                "    fmt='.2f',\n",
                "    cmap='Blues',\n",
                "    xticklabels=['Pos','Neu','Neg'],\n",
                "    yticklabels=['Pos','Neu','Neg']\n",
                ")\n",
                "plt.ylabel('Actual')\n",
                "plt.xlabel('Predicted')\n",
                "\n",
                "warning = '' if cm_dict['threshold_passed'] else ' - Warning: Off-diagonal > 15%'\n",
                "plt.title('Confusion Matrix (normalized)' + warning)\n",
                "plt.tight_layout()\n",
                "plt.savefig('sentiment_confusion.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "cm_dict\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sentiment-Price Correlation\n",
                "\n",
                "Target: |rho| >= 0.15"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06494155",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sentiment-Price correlation analysis (daily aggregation)\n",
                "# Aggregate sentiment by date\n",
                "sent_df = test_df.copy()\n",
                "sent_df['date'] = pd.to_datetime(sent_df['published_at']).dt.normalize()\n",
                "sent_df['continuous_senti'] = continuous_sentiment\n",
                "\n",
                "daily_sent = sent_df.groupby('date')['continuous_senti'].mean().sort_index()\n",
                "\n",
                "# Compute equal-weight market daily returns from price data\n",
                "p = prices_df.copy()\n",
                "p['date'] = pd.to_datetime(p['Date']).dt.normalize()\n",
                "p = p.sort_values(['Ticker','date'])\n",
                "p['ret_1d'] = p.groupby('Ticker')['Close'].pct_change()\n",
                "\n",
                "mkt_ret = p.groupby('date')['ret_1d'].mean().dropna().sort_index()\n",
                "\n",
                "# Align by date index\n",
                "aligned = pd.concat([daily_sent, mkt_ret], axis=1, join='inner').dropna()\n",
                "aligned.columns = ['sent', 'ret']\n",
                "\n",
                "lags = [1, 2, 3, 5, 7]\n",
                "price_corr = sentiment_price_correlation(aligned['sent'], aligned['ret'], lags=lags)\n",
                "\n",
                "# Plot correlations by lag\n",
                "lag_vals = []\n",
                "corr_vals = []\n",
                "pvals = []\n",
                "for l in lags:\n",
                "    key = f'lag_{l}'\n",
                "    res = price_corr.get(key, {'correlation': 0.0, 'p_value': 1.0})\n",
                "    lag_vals.append(l)\n",
                "    corr_vals.append(res['correlation'])\n",
                "    pvals.append(res['p_value'])\n",
                "\n",
                "plt.figure(figsize=(6,4))\n",
                "bar = plt.bar(lag_vals, corr_vals, color=['#4e79a7' if abs(c) >= 0.15 and p < 0.05 else '#a0cbe8' for c,p in zip(corr_vals,pvals)])\n",
                "plt.axhline(0, color='gray', linewidth=0.8)\n",
                "plt.title('Sentiment vs Forward Returns Correlation by Lag (daily, eq-weight)')\n",
                "plt.xlabel('Forward lag (days)')\n",
                "plt.ylabel('Pearson correlation')\n",
                "for i, (c, p) in enumerate(zip(corr_vals, pvals)):\n",
                "    plt.text(lag_vals[i], c + np.sign(c)*0.01, f\"p={p:.3f}\", ha='center', va='bottom' if c>=0 else 'top', fontsize=8)\n",
                "plt.tight_layout()\n",
                "plt.savefig('sentiment_price_correlation.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "significant_lags = [l for l, c, p in zip(lag_vals, corr_vals, pvals) if abs(c) >= 0.15 and p < 0.05]\n",
                "print('Significant lags (|rho|>=0.15, p<0.05):', significant_lags)\n",
                "\n",
                "price_corr\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "874c1d5c",
            "metadata": {},
            "source": [
                "## 6. Baseline Comparison\n",
                "\n",
                "VADER/TextBlob baseline vs model; report improvement and McNemar test p-value (if available).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f98c8f13",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Baseline comparison (VADER/TextBlob if available, else simple lexicon)\n",
                "try:\n",
                "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
                "    analyzer = SentimentIntensityAnalyzer()\n",
                "    baseline_scores = [analyzer.polarity_scores(t)['compound'] for t in test_texts]\n",
                "    def score_to_class(x):\n",
                "        return 0 if x >= 0.05 else (2 if x <= -0.05 else 1)\n",
                "    baseline_pred = np.array([score_to_class(s) for s in baseline_scores], dtype=int)\n",
                "    baseline_name = 'VADER'\n",
                "except Exception:\n",
                "    try:\n",
                "        from textblob import TextBlob\n",
                "        def tb_score(text):\n",
                "            return TextBlob(text).sentiment.polarity\n",
                "        baseline_scores = [tb_score(t) for t in test_texts]\n",
                "        def score_to_class(x):\n",
                "            return 0 if x >= 0.05 else (2 if x <= -0.05 else 1)\n",
                "        baseline_pred = np.array([score_to_class(s) for s in baseline_scores], dtype=int)\n",
                "        baseline_name = 'TextBlob'\n",
                "    except Exception:\n",
                "        # Simple lexicon fallback\n",
                "        pos_words = {'beat','surge','gain','strong','growth','bullish','upgrade','outperform'}\n",
                "        neg_words = {'miss','fall','loss','weak','decline','bearish','downgrade','underperform'}\n",
                "        def lex_score(text):\n",
                "            tokens = str(text).lower().split()\n",
                "            pos = sum(w in pos_words for w in tokens)\n",
                "            neg = sum(w in neg_words for w in tokens)\n",
                "            return (pos - neg) / max(1, (pos + neg))\n",
                "        baseline_scores = [lex_score(t) for t in test_texts]\n",
                "        def score_to_class(x):\n",
                "            return 0 if x > 0 else (2 if x < 0 else 1)\n",
                "        baseline_pred = np.array([score_to_class(s) for s in baseline_scores], dtype=int)\n",
                "        baseline_name = 'Lexicon'\n",
                "\n",
                "comp = compare_to_baseline(test_labels, y_pred, baseline_pred, metric='f1')\n",
                "print({'baseline': baseline_name, **comp})\n",
                "\n",
                "# p_value and test_stat are now included in comp from compare_to_baseline()\n",
                "print(f\"McNemar's test: statistic={comp['test_stat']:.4f}, p-value={comp['p_value']:.4f}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1deee78c",
            "metadata": {},
            "source": [
                "## 7. Error Analysis\n",
                "\n",
                "Inspect common failure modes and sample misclassifications.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1935bbc7",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "6a7ff3fd",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
