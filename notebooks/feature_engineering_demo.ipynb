{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2 Feature Engineering Demo\n",
        "\n",
        "This notebook walks through the Phase 2 pipeline for technical indicators, windowed sequences, and FinBERT embeddings described in `docs/metrics_and_evaluation.md`. It uses the same utilities as the CLI scripts (`scripts/feature_engineering.py` and `scripts/text_embeddings.py`) to ensure consistency and reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Repo imports\n",
        "import sys\n",
        "sys.path.insert(0, str(Path('..').resolve()))\n",
        "\n",
        "from backend.utils.config import config\n",
        "from backend.utils.file_operations import get_latest_file, load_existing_data\n",
        "from backend.utils.preprocessing import (\n",
        "    temporal_train_test_split,\n",
        "    fit_scaler,\n",
        "    transform_with_scaler,\n",
        "    validate_no_leakage,\n",
        ")\n",
        "from scripts.feature_engineering import (\n",
        "    compute_technical_indicators,\n",
        "    create_windowed_sequences,\n",
        "    split_and_scale,\n",
        ")\n",
        "from scripts import text_embeddings as text_embed\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "Define the ticker, data directories, and defaults inherited from `.env` via `backend.utils.config.Config`. The notebook automatically searches for the most recent raw snapshot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TICKER = \"RELIANCE.NS\"\n",
        "RAW_PRICES_DIR = Path(\"../data/raw/prices\").resolve()\n",
        "RAW_NEWS_DIR = Path(\"../data/raw/news\").resolve()\n",
        "PROCESSED_DIR = Path(\"../data/processed\").resolve()\n",
        "LOOKBACK = config.lookback_window\n",
        "\n",
        "\n",
        "def load_latest_price_df(ticker: str) -> pd.DataFrame:\n",
        "    sanitized = ticker.replace(\".\", \"_\")\n",
        "    latest_file = get_latest_file(RAW_PRICES_DIR, f\"*/{sanitized}.csv\")\n",
        "    if latest_file is None:\n",
        "        print(\"⚠️ No raw price snapshot found. Generating synthetic demo data.\")\n",
        "        dates = pd.date_range(\"2022-01-01\", periods=400, freq=\"B\")\n",
        "        base = np.linspace(900, 1400, num=len(dates))\n",
        "        noise = np.random.default_rng(42).normal(0, 15, len(dates))\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"date\": dates,\n",
        "                \"open\": base + noise,\n",
        "                \"high\": base + noise + 10,\n",
        "                \"low\": base + noise - 10,\n",
        "                \"close\": base + noise / 2,\n",
        "                \"volume\": np.linspace(1e6, 5e6, len(dates)),\n",
        "            }\n",
        "        )\n",
        "        return df\n",
        "    df = load_existing_data(latest_file, file_format=\"csv\")\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "price_df = load_latest_price_df(TICKER)\n",
        "price_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(price_df[\"date\"], price_df[\"close\"], label=\"Close\", color=\"#2563eb\")\n",
        "ax.set_title(f\"{TICKER} Close Price\")\n",
        "ax.set_ylabel(\"Price (INR)\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Technical Indicators\n",
        "\n",
        "Use the helper from `scripts/feature_engineering.py` (`compute_technical_indicators`) so the notebook stays aligned with the CLI workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indicator_df, indicator_cols = compute_technical_indicators(price_df, config.technical_indicators)\n",
        "print(f\"Computed indicators: {indicator_cols}\")\n",
        "indicator_df.tail().head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
        "latest = indicator_df.tail(200)\n",
        "\n",
        "# Price + Bollinger\n",
        "axes[0].plot(latest[\"date\"], latest[\"close\"], label=\"Close\", color=\"#0ea5e9\")\n",
        "if {\"bb_upper_20\", \"bb_middle_20\", \"bb_lower_20\"}.issubset(latest.columns):\n",
        "    axes[0].plot(latest[\"date\"], latest[\"bb_upper_20\"], label=\"BB Upper\", linestyle=\"--\", color=\"#f97316\")\n",
        "    axes[0].plot(latest[\"date\"], latest[\"bb_middle_20\"], label=\"BB Mid\", linestyle=\":\", color=\"#22c55e\")\n",
        "    axes[0].plot(latest[\"date\"], latest[\"bb_lower_20\"], label=\"BB Lower\", linestyle=\"--\", color=\"#f97316\")\n",
        "axes[0].set_title(\"Close with Bollinger Bands\")\n",
        "axes[0].legend()\n",
        "\n",
        "# RSI\n",
        "if \"rsi_14\" in latest.columns:\n",
        "    axes[1].plot(latest[\"date\"], latest[\"rsi_14\"], color=\"#f43f5e\")\n",
        "    axes[1].axhline(70, color=\"#94a3b8\", linestyle=\"--\")\n",
        "    axes[1].axhline(30, color=\"#94a3b8\", linestyle=\"--\")\n",
        "    axes[1].set_title(\"RSI (14)\")\n",
        "    axes[1].set_ylim(0, 100)\n",
        "\n",
        "# MACD\n",
        "macd_cols = {\"macd_line\", \"macd_signal\", \"macd_hist\"}\n",
        "if macd_cols.issubset(latest.columns):\n",
        "    axes[2].plot(latest[\"date\"], latest[\"macd_line\"], label=\"MACD\", color=\"#6366f1\")\n",
        "    axes[2].plot(latest[\"date\"], latest[\"macd_signal\"], label=\"Signal\", color=\"#22d3ee\")\n",
        "    axes[2].bar(latest[\"date\"], latest[\"macd_hist\"], label=\"Hist\", color=\"#a855f7\")\n",
        "    axes[2].set_title(\"MACD (12-26-9)\")\n",
        "    axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Val/Test Split & Scaling\n",
        "\n",
        "We reuse `temporal_train_test_split`, `fit_scaler`, and `split_and_scale` to guarantee the notebook honors the same leakage constraints as the CLI. This cell reports split sizes and window shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_cols = [col for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"] if col in indicator_df.columns]\n",
        "feature_cols += indicator_cols\n",
        "(\n",
        "    split_tensors,\n",
        "    split_targets,\n",
        "    split_indices,\n",
        "    split_ranges,\n",
        "    feature_names,\n",
        "    scaler,\n",
        "    window_counts,\n",
        ") = split_and_scale(\n",
        "    indicator_df,\n",
        "    feature_cols=feature_cols,\n",
        "    lookback=LOOKBACK,\n",
        "    train_ratio=config.train_split_ratio,\n",
        "    val_ratio=config.val_split_ratio,\n",
        "    test_ratio=config.test_split_ratio,\n",
        "    scaler_type=config.scaler_type,\n",
        ")\n",
        "\n",
        "print(\"Window counts:\", json.dumps(window_counts, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_train_window = split_tensors[\"train\"][0]\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.heatmap(sample_train_window.T, cmap=\"mako\", cbar=True)\n",
        "plt.title(\"Sample Train Window (features × timesteps)\")\n",
        "plt.xlabel(\"Timesteps (days)\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Leakage Validation\n",
        "\n",
        "Per docs §4.3, validation/test samples must chronologically follow the training period. The helper will raise if indices overlap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_df, splits = temporal_train_test_split(indicator_df)\n",
        "validate_no_leakage(sorted_df, splits)\n",
        "print(\"✅ Temporal ordering check passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Scaling Diagnostics\n",
        "\n",
        "Fit the scaler on training rows only, then compare original vs scaled distributions to confirm values are bounded and centered as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_slice = indicator_df.iloc[split_indices[\"train\"][\"start_idx\"]:split_indices[\"train\"][\"end_idx\"]]\n",
        "scaler = fit_scaler(train_slice[feature_cols], scaler_type=config.scaler_type)\n",
        "train_scaled = transform_with_scaler(scaler, train_slice[feature_cols])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].hist(train_slice[\"close\"], bins=30, color=\"#f59e0b\")\n",
        "axes[0].set_title(\"Raw Close Distribution\")\n",
        "axes[1].hist(train_scaled[:, feature_cols.index(\"close\")], bins=30, color=\"#10b981\")\n",
        "axes[1].set_title(\"Scaled Close Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. FinBERT Embeddings (News)\n",
        "\n",
        "To avoid long downloads during demos, the cell attempts to load the ProsusAI/finbert checkpoint. If unavailable, it falls back to dummy vectors so downstream shapes still match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_news_records(ticker: str):\n",
        "    sanitized = ticker.replace(\".\", \"_\")\n",
        "    latest_file = get_latest_file(RAW_NEWS_DIR, f\"*/{sanitized}_news.json\")\n",
        "    if latest_file is None:\n",
        "        print(\"⚠️ No raw news snapshot found. Using synthesized headlines for demo.\")\n",
        "        now = datetime.utcnow().isoformat()\n",
        "        return [\n",
        "            {\"title\": \"Reliance gains on earnings\", \"description\": \"Q2 beat estimates\", \"published_at\": now, \"url\": \"demo-1\"},\n",
        "            {\"title\": \"Oil prices dip\", \"description\": \"Potential drag on refining margins\", \"published_at\": now, \"url\": \"demo-2\"},\n",
        "        ]\n",
        "    data = load_existing_data(latest_file, file_format=\"json\")\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        return data.to_dict(\"records\")\n",
        "    return data\n",
        "\n",
        "\n",
        "news_records = load_news_records(TICKER.split(\".\")[0])\n",
        "news_samples = text_embed.prepare_news_samples(news_records, ticker=TICKER.split(\".\")[0])\n",
        "samples_subset = news_samples[: min(32, len(news_samples))]\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    tokenizer, embed_model, classifier, label_map = text_embed.load_models(\n",
        "        config.finbert_model_name,\n",
        "        device=device,\n",
        "        classify=False,\n",
        "    )\n",
        "    news_embeddings, _, _ = text_embed.generate_embeddings(\n",
        "        samples_subset,\n",
        "        tokenizer=tokenizer,\n",
        "        embed_model=embed_model,\n",
        "        classifier=None,\n",
        "        label_map=None,\n",
        "        device=device,\n",
        "        batch_size=min(config.finbert_batch_size, len(samples_subset)),\n",
        "        max_length=config.finbert_max_length,\n",
        "    )\n",
        "except Exception as exc:  # noqa: BLE001\n",
        "    print(f\"⚠️ FinBERT unavailable ({exc}); using random demo embeddings instead.\")\n",
        "    news_embeddings = np.random.normal(size=(len(samples_subset), 768))\n",
        "\n",
        "print(\"Embedding matrix shape:\", news_embeddings.shape)\n",
        "\n",
        "if len(samples_subset) >= 2:\n",
        "    pca = PCA(n_components=2)\n",
        "    coords = pca.fit_transform(news_embeddings)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.scatter(coords[:, 0], coords[:, 1], c=np.arange(len(coords)), cmap=\"viridis\")\n",
        "    plt.title(\"News Embeddings (PCA)\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Metadata Snapshot\n",
        "\n",
        "Feature + embedding scripts persist metadata JSON files. The snippet below shows the structure stored inside `data/processed/{ticker}/YYYY-MM-DD/metadata.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata_preview = {\n",
        "    \"ticker\": TICKER,\n",
        "    \"lookback_window\": LOOKBACK,\n",
        "    \"split_indices\": split_indices,\n",
        "    \"split_date_ranges\": split_ranges,\n",
        "    \"window_counts\": window_counts,\n",
        "    \"indicators\": indicator_cols,\n",
        "    \"scaler\": {\"type\": config.scaler_type, \"n_features\": scaler.n_features_in_},\n",
        "}\n",
        "print(json.dumps(metadata_preview, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary & Next Steps\n",
        "\n",
        "- Technical indicators, splits, scaling, and window generation mirror `scripts/feature_engineering.py`.\n",
        "- FinBERT embeddings (with fallback) demonstrate how `scripts/text_embeddings.py` feeds downstream sentiment-aware models.\n",
        "- Metadata captures split indices, date ranges, scaler params, and embedding paths for leakage audits.\n",
        "\n",
        "Next: feed `train_features.npy` / embeddings into the Phase 3 model notebooks once modeling scripts are implemented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
