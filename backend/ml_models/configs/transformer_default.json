{
    "model_type": "transformer",
    "input_dim": null,
    "d_model": 128,
    "nhead": 4,
    "num_layers": 2,
    "dim_feedforward": 512,
    "dropout": 0.2,
    "patch_len": 12,
    "forecast_horizon": 7,
    "learning_rate": 0.001,
    "batch_size": 32,
    "max_epochs": 100,
    "early_stopping_patience": 10,
    "optimizer": "adam",
    "loss_function": "mse",
    "scheduler": "reduce_lr_on_plateau",
    "scheduler_params": {
        "mode": "min",
        "factor": 0.5,
        "patience": 5
    },
    "_comment": "patch_len=12 divides lookback_window=60 evenly (5 patches), optimizing for PatchTST architecture"
}